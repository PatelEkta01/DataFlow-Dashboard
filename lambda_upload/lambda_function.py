# import boto3
# import csv
# import uuid
# import os
# import time
# import datetime  # ‚è∞ Add time module to store timestamps

# sns = boto3.client('sns')
# TOPIC_ARN = 'arn:aws:sns:us-east-1:314310821739:dataflow-etl-summary'

# s3 = boto3.client('s3')
# dynamodb = boto3.resource('dynamodb')
# table_name = os.environ['DDB_TABLE']
# table = dynamodb.Table(table_name)

# def lambda_handler(event, context):
#     print("‚úÖ Received event:", event)

#     bucket = event['Records'][0]['s3']['bucket']['name']
#     key = event['Records'][0]['s3']['object']['key']
#     filename = key.split('/')[-1]  # e.g., "data.csv"
#     upload_time = int(time.time())  # üïì Add upload timestamp

#     obj = s3.get_object(Bucket=bucket, Key=key)
#     lines = obj['Body'].read().decode('utf-8').splitlines()
#     reader = csv.DictReader(lines)

#     if reader.fieldnames is None or None in reader.fieldnames:
#         print("‚ùå Invalid CSV: missing or malformed headers.")
#         return {"statusCode": 400, "body": "CSV file missing valid headers."}

#     count = 0
#     skipped = 0  # ‚úÖ Count malformed rows

#     for row in reader:
#         if not isinstance(row, dict) or None in row.keys():
#             print("‚ö†Ô∏è Skipping malformed row:", row)
#             skipped += 1  # ‚úÖ Track skipped rows
#             continue

#         row['record_id'] = str(uuid.uuid4())
#         row['file_name'] = filename
#         row['upload_time'] = upload_time  # üÜï Add timestamp
#         table.put_item(Item=row)
#         count += 1

#     # ‚úÖ Send Enhanced ETL summary to SNS (non-blocking)
#     try:
#         sns.publish(
#             TopicArn=TOPIC_ARN,
#             Subject="‚úÖ ETL Process Completed with Technical Details",
#             Message=(
#                 f"‚úÖ ETL Summary - Technical Report\n"
#                 f"File: {filename}\n"
#                 f"Bucket: {bucket}\n"
#                 f"Key: {key}\n"
#                 f"Lambda ID: {context.aws_request_id}\n"
#                 f"Upload Time: {datetime.datetime.utcnow().isoformat()} UTC\n\n"
#                 f"--- ETL Operations ---\n"
#                 f"‚Ä¢ Total rows processed: {count}\n"
#                 f"‚Ä¢ Skipped rows (malformed): {skipped}\n"
#                 f"‚Ä¢ UUIDs generated: {count}\n"
#                 f"‚Ä¢ Fields added: record_id, file_name, upload_time\n"
#                 f"‚Ä¢ Timestamp format: UNIX (epoch seconds)\n"
#                 f"‚Ä¢ Headers in file: {', '.join(reader.fieldnames)}"
#             )
#         )
#     except Exception as e:
#         print("‚ö†Ô∏è SNS publish failed:", e)

#     return {
#         "statusCode": 200,
#         "body": f"‚úÖ {count} rows from {filename} processed successfully."
#     }
import boto3
import csv
import uuid
import os
import time
import datetime 
from decimal import Decimal, InvalidOperation


sns = boto3.client('sns')
TOPIC_ARN = 'arn:aws:sns:us-east-1:314310821739:dataflow-etl-summary'

s3 = boto3.client('s3')
dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table(os.environ['DDB_TABLE'])

def lambda_handler(event, context):
    print("‚úÖ Received event:", event)

    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']
    filename = key.split('/')[-1]
    upload_time = int(time.time())

    obj = s3.get_object(Bucket=bucket, Key=key)
    lines = obj['Body'].read().decode('utf-8').splitlines()
    reader = csv.DictReader(lines)

    if reader.fieldnames is None or None in reader.fieldnames:
        print("‚ùå Invalid CSV: missing or malformed headers.")
        return {"statusCode": 400, "body": "CSV file missing valid headers."}

    count = 0
    skipped = 0

    for row in reader:
        # üßº Normalize keys and trim values
        row = {
            k.lower(): v.strip() if isinstance(v, str) else v
            for k, v in row.items()
        }

        notes = []

        # ‚úÖ Handle missing fields
        if not row.get('name'):
            row['name'] = "Unknown"
            notes.append("missing name")
        if not row.get('email'):
            row['email'] = "noemail@example.com"
            notes.append("missing email")

        # ‚úÖ Clean + validate amount
        try:
             row['amount'] = Decimal(row.get('amount', '').strip())
        except (InvalidOperation, ValueError, TypeError, AttributeError):
            row['amount'] = Decimal("0.0")
        notes.append("invalid or missing amount, set to 0.0")

        # ‚úÖ Normalize fields
        row['name'] = row['name'].title()
        row['email'] = row['email'].lower()

        # ‚úÖ Enrich with system metadata
        row['record_id'] = str(uuid.uuid4())
        row['file_name'] = filename
        row['upload_time'] = upload_time
        row['etl_notes'] = ", ".join(notes) if notes else "clean"

        try:
            table.put_item(Item=row)
            count += 1
        except Exception as e:
            print(f"‚ùå Failed to insert row: {e}")
            skipped += 1
            continue

    # üì¨ Send ETL summary via SNS
    try:
       sns.publish(
    TopicArn=TOPIC_ARN,
    Subject="‚úÖ ETL Process Completed with Technical Cleanup",
    Message=(
        f"‚úÖ ETL Summary - Technical Cleanup Report\n"
        f"File: {filename}\n"
        f"Bucket: {bucket}\n"
        f"Key: {key}\n"
        f"Lambda ID: {context.aws_request_id}\n"
        f"Upload Time: {datetime.datetime.utcnow().isoformat()} UTC\n\n"
        f"--- ETL Transformations ---\n"
        f"‚Ä¢ Total rows inserted: {count}\n"
        f"‚Ä¢ Rows skipped (DynamoDB write failures): {skipped}\n"
        f"‚Ä¢ Fields cleaned: name ‚Üí title case, email ‚Üí lowercase\n"
        f"‚Ä¢ amount: converted to Decimal, defaulted to 0.0 if invalid\n"
        f"‚Ä¢ Field 'etl_notes' tracks cleaning actions per row\n"
        f"‚Ä¢ Metadata added: record_id, file_name, upload_time\n"
        f"‚Ä¢ Timestamp format: UNIX epoch\n"
        f"‚Ä¢ Original headers: {', '.join(reader.fieldnames)}"
    )
)

    except Exception as e:
        print("‚ö†Ô∏è SNS publish failed:", e)

    return {
        "statusCode": 200,
        "body": f"‚úÖ {count} rows from {filename} cleaned and processed successfully."
    }
